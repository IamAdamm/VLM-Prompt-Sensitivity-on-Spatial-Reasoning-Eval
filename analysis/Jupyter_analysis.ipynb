{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af724c2e",
   "metadata": {},
   "source": [
    "# Evaluating Prompt Sensitivity for VLMs on Simple Spatial Reasoning: A Controlled Study on LLaVA\n",
    "###### 20.12.2025\n",
    "#### Notebook by [Adam Astamir](https://adamastamir.vercel.app/)\n",
    "##### Supported by [Jae Hee Lee](https://jaeheelee.gitlab.io/)\n",
    "##### [University of Hamburg](https://www.uni-hamburg.de/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d60bf9",
   "metadata": {},
   "source": [
    "| Paper | Model(s) | Task Type | Prompt Role | Key Failure | What’s Missing |\n",
    "|------|----------|-----------|-------------|-------------|---------------|\n",
    "| What’s “Up” with Vision-Language Models? (2023) | 18 VLMs (CLIP, BLIP, BLIP-2, XVLM, CoCa, FLAVA...) | Image–text matching for basic spatial relations (left/right, on/under, in front/behind) | Minimal. Prompting controlled for isolation of spatial reasoning | Models fail to reliably distinguish basic spatial relations. performance often near random |  Focus on prompting not given. Also no Counting involved   |\n",
    "| Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models (2024)| 7 VLMs (GPT-4V, **LLaVA**, BLIP-2, MiniGPT-4, OpenFlamingo, InstructBLIP, Otter) |  Multiple-choice spatial reasoning tasks (e.g., map navigation, object arrangement, grid puzzles)  | Moderate. Text prompts guide reasoning, images provide context | VLMs often ignore visual input or rely too heavily on text. accuracy sometimes at or below random chance; struggles with multi-step spatial reasoning  | No experiments on CLIP specifically. Synthetic image used may not include biases of real world examples  |\n",
    "| Enhancing Spatial Reasoning in Vision-Language Models via CoT Prompting & RL (2025) | PaLI-Gemma2, Qwen2.5-VL, Llama-4-Scout |  Counting, Relations, Depth, Distance, Egocentric/Object Movement (SAT, CV-Bench, CLEVR, VSR) | All about prompting. Many different techniques |  Naïve CoT can harm performance; models may rely on superficial linguistic patterns; standard SFT fails to generalize OOD | No LlaVA and little CLIP metion  |\n",
    "| Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas (2025) | **LLaVA-1.5, LLaVA-1.6**, Qwen2-VL | Simple Multiple-choice spatial reasoning tasks (left/right, on/under, front/behind) from WhatsUp & VSR datasets  | Moderate. Prompts ask for object relations; adaptive attention (ADAPTVIS) uses model confidence to guide reasoning |  VLMs underutilize image tokens; attention misalignment causes spatial hallucinations; model over-relies on familiar relationships; low-confidence relations fail |  Focus on CLIP as backbone is limited; dependency on validation sets for ADAPTVIS; only intermediate layers analyzed; specific prompt effects not deeply varied |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e2ff4",
   "metadata": {},
   "source": [
    "## Abstract "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e6635",
   "metadata": {},
   "source": [
    "Vision–Language Models (VLMs), including LLaVA, have advanced considerably in multimodal understanding, but they continue to exhibit weaknesses in simple spatial reasoning tasks, such as identifying whether an object is to the left, right, above, or below another object. This study investigates how sensitive VLM performance is to prompt design, including prompt structure, option ordering, and wording. We hypothesize that spatial reasoning accuracy significantly varies depending on prompt clarity and phrasing, with clear, well-structured prompts yielding higher performance than ambiguous ones. To test this, we use part of the WhatsUp dataset, which contains 205 images presented in four controlled spatial positions, totaling 820 images, out of which we use 418 from control group 2. The dataset isolates basic spatial relations while minimizing background biases and confounding object-context associations. We evaluate multiple prompting strategies, including baseline multiple-choice prompts, we test for shuffled and wording-variant prompts, Chain-of-Thought (CoT) prompting, Scene-Graph-based CoT and multi-stage descriptive prompts. Accuracy, prompt sensitivity, and error patterns are measured to assess how subtle variations in prompt design influence VLM reasoning. Preliminary analyses indicate that even small changes in phrasing or option ordering can have notable effects on model performance, supporting the hypothesis that VLM spatial reasoning is highly prompt-sensitive. The study aims to provide a controlled, systematic evaluation of prompt effects on spatial reasoning, quantify performance variations, and identify strategies that reliably improve VLM reasoning in simple spatial tasks.  (KEY FINDINGS MISSING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1853b",
   "metadata": {},
   "source": [
    "## Literature Review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f696f",
   "metadata": {},
   "source": [
    "Spatial reasoning presents a persistent challenge for vision-language models (VLMs) because it requires the integration of visual information with textual cues in a coherent way. Many studies have shown that models like LLaVA tend to over-rely on textual prompts, often underutilizing the visual context. In some cases, performance actually improves when visual input is removed, highlighting the difficulty these models face in correctly aligning image representations with language understanding. Misaligned attention mechanisms further contribute to errors, leading to what has been described as “spatial hallucinations,” where objects’ relative positions are misinterpreted even in simple, controlled scenes.\n",
    "\n",
    "The WhatsUp dataset, introduced in 2023, provides a benchmark for evaluating basic spatial relations, such as left/right, on/under, and in front/behind. Its controlled design minimizes background distractions and isolates object positions, making it ideal for assessing prompt sensitivity. Empirical results on WhatsUp indicate that VLMs often perform near chance levels, around 50%, and subtle variations in wording can have a substantial effect on accuracy. For example, asking about an object “behind” another yields only 52% accuracy, whereas phrasing it as “in the background” increases performance to 67%. These results illustrate how linguistic framing alone can significantly influence model behavior, even for straightforward spatial tasks.\n",
    "\n",
    "Further evidence comes from the SpatialEval study, which shows that LLaVA and other VLMs frequently rely more on textual than visual information, often ignoring image content entirely. The study demonstrates that prompt phrasing and structure materially affect model performance, underscoring the importance of controlled experiments to isolate the effect of prompt design.\n",
    "\n",
    "Attempts to improve reasoning through Chain-of-Thought (CoT) prompting—guiding the model to think step by step—have yielded mixed results. Naive CoT prompts often fail to improve performance and can even degrade accuracy. In contrast, approaches that explicitly separate perception from reasoning, such as Scene-Graph-based CoT or multi-stage descriptive prompting, have been shown to enhance spatial understanding significantly. By first describing all objects and their spatial relationships, these methods help models interpret visual information more effectively, reducing errors caused by superficial linguistic biases.\n",
    "\n",
    "Recent work examining LLaVA’s attention patterns offers additional insight. Despite image tokens comprising approximately 90% of the input, they receive only about 10% of the model’s attention, indicating a strong bias toward textual priors. Techniques such as ADAPTVIS, which use confidence-guided attention, can partially correct this imbalance and improve spatial reasoning accuracy. This highlights that prompt design alone is insufficient; effective evaluation of spatial reasoning also requires attention to how visual information is processed internally.\n",
    "\n",
    "Overall, the literature suggests that spatial reasoning in VLMs is highly sensitive to prompt phrasing, structure, and ordering. Unstructured or ambiguous prompts frequently lead to poor performance, whereas structured, multi-stage, or descriptive prompts improve outcomes by explicitly guiding models to consider visual and spatial relationships. However, there remains a lack of systematic investigation into how multiple-choice variations, ordering, and alternative wordings influence simple spatial reasoning in LLaVA. This gap motivates the controlled experiments presented in this work, focusing on isolating the effects of prompt design on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a8422",
   "metadata": {},
   "source": [
    "## Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de177cac",
   "metadata": {},
   "source": [
    "Our initial experiments focused exclusively on multiple-choice prompts to identify a strong baseline before testing more complex prompting strategies. Using the WhatsUp controlled dataset images, we evaluated variations in prompt wording and the ordering of multiple-choice options. This approach allows us to isolate the effects of text phrasing and option positioning on LLaVA’s spatial reasoning.\n",
    "\n",
    "We tested a range of prompting strategies including simple baseline instructions (“Which object is to the left of X?”), alternative wordings (\"under\" versus “underneath\", “above” versus “on top”), and shuffled answer options. Our goal was to determine which combination of wording and ordering yields the highest accuracy so that subsequent experiments with Chain-of-Thought, Scene-Graph-based CoT, multi-stage descriptive prompting, and few-shot prompting can be compared to a strong multiple-choice baseline and use the most performant option of order and wording. Metrics for evaluation included accuracy, prompt sensitivity, confidence, and error type. All experiments were run with a batch size of 32 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e47e10",
   "metadata": {},
   "source": [
    "## Actual Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e1a03",
   "metadata": {},
   "source": [
    "Testing LLaVA with multiple-choice prompts and their variations, we observed a wide range of accuracies, indicating significant prompt sensitivity. The baseline multiple-choice prompt achieved 58.37% accuracy, while shuffling the answer options reduced performance to 45.93%. Some combinations of shuffled options and altered wording, such as “Shuffle Vary Above,” produced extremely low accuracy at 26.08%, showing that subtle changes in both phrasing and ordering can drastically impair performance. Other variations, such as “Shuffle Vary Below” or “Shuffle Vary Under,” achieved 48.33% and 54.55%, respectively. Alternative wording alone, without shuffling, resulted in modest changes compared to the baseline, with “Vary Above” at 55.02%, “Vary Below” at 54.07%, and “Vary Under” at 54.31%.\n",
    "\n",
    "These results indicate that LLaVA is highly sensitive to option order and wording, with shuffling having a particularly strong negative impact. The model appears to exhibit a “first-option bias,” relying on position rather than true spatial reasoning. While alternative wording alone slightly affects performance, ordering interacts with phrasing to produce complex effects. Overall, the model struggles with simple spatial reasoning, and performance can swing from near random to moderately above chance depending solely on prompt design.Experimetn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84450794",
   "metadata": {},
   "source": [
    "## Results & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f27080",
   "metadata": {},
   "source": [
    "Across all evaluated prompt variants, LLaVA achieved an overall accuracy of 49.58%, with an average reported confidence of 47.56%. Given the binary nature of the spatial reasoning task, this overall performance lies close to random chance. However, aggregate accuracy masks substantial variation across prompt configurations, indicating systematic sensitivity to prompt structure rather than random guessing.\n",
    "\n",
    "![Accuracy 49.58%](MultipleChoiceAccuracy.png)\n",
    "\n",
    "**Effect of Multiple-Choice Formatting and Option Order**\n",
    "\n",
    "The standard multiple-choice format achieved the highest accuracy (58.37%), suggesting that under stable and familiar prompt structures, LLaVA can perform above chance on simple spatial reasoning tasks. In contrast, shuffling the order of answer options led to a notable drop in accuracy (45.93%), despite a slightly higher average confidence (50.54%). This degradation implies that answer option order plays a significant role in model behavior.\n",
    "\n",
    "The impact of shuffling becomes particularly pronounced when combined with subtle wording changes. The “Multiple Choice Shuffle Vary Above” condition yielded an accuracy of only 26.08%, far below random chance, while simultaneously exhibiting the highest confidence across all conditions (59.96%). Such performance cannot be explained by uncertainty or noise; instead, it indicates the consistent application of an incorrect decision rule.\n",
    "\n",
    "Importantly, this effect is not uniform across all wording variations. While shuffling combined with “below” wording resulted in near-chance performance (48.33%), shuffling combined with “under” performed substantially better (54.55%). This non-linear interaction suggests that option order and linguistic phrasing jointly influence the model’s internal heuristics.\n",
    "\n",
    "**Effect of Wording Without Shuffling**\n",
    "\n",
    "When alternative spatial wording was introduced without shuffling the answer options, performance remained relatively stable. Accuracy across the “Vary Above”, “Vary Below”, and “Vary Under” conditions ranged from 54.07% to 55.02%, only slightly below the baseline multiple-choice condition. This indicates that wording variations alone do not severely impair performance, provided the overall prompt structure remains familiar.\n",
    "\n",
    "Taken together, these findings suggest that LLaVA is more sensitive to disruptions in answer ordering than to semantic variations in spatial terminology. Crucially, it is the interaction between wording and ordering—not either factor in isolation—that produces the most severe failure modes.\n",
    "\n",
    "**Confidence–Accuracy Mismatch and Systematic Failure**\n",
    "\n",
    "One of the most striking observations is the divergence between confidence and accuracy in certain conditions. The lowest-performing configuration (“Multiple Choice Shuffle Vary Above”) exhibits the highest confidence, indicating that the model is not merely guessing but is confidently applying an incorrect heuristic. This pattern suggests that model confidence reflects internal linguistic certainty rather than successful visual grounding.\n",
    "\n",
    "Accuracy falling well below chance further supports this interpretation. If the model were uncertain, performance would be expected to regress toward random guessing. Instead, the observed results indicate systematic misclassification driven by consistent but flawed decision rules.\n",
    "\n",
    "**Hypothesis: Interaction Between Spatial Direction and First-Option Bias**\n",
    "\n",
    "A plausible explanation for the observed sensitivity to shuffling involves positional bias in multiple-choice selection. Prior work on language models has documented a tendency to favor the first option in a list. In the non-shuffled baseline condition, the correct answer may more frequently correspond to the first option when the spatial relation involves “left”, due to the natural ordering of answer choices.\n",
    "\n",
    "Under this hypothesis, shuffling disrupts an implicit alignment between spatial direction (e.g., “left”) and option position, causing the model to apply a learned positional heuristic that no longer corresponds to the correct answer. This could explain why shuffling alone reduces performance, and why the combination of shuffling with specific wording (notably “above”) leads to severe degradation.\n",
    "\n",
    "Importantly, this interpretation predicts that the observed failure is not due to an inability to understand spatial relations per se, but rather due to the model over-relying on linguistic and positional shortcuts that correlate with correctness in the unshuffled setting.\n",
    "\n",
    "**Ground Truth Distribution and Positional Bias Analysis**\n",
    "\n",
    "To evaluate whether the observed degradation under shuffled answer options could be explained by dataset bias, we analyzed the distribution of ground truth labels across all test cases. The dataset is nearly perfectly balanced, with each spatial relation (“left”, “right”, “on top”, “under”) occurring in approximately 25% of samples.\n",
    "\n",
    "This balance rules out label frequency as a confounding factor. If shuffling merely disrupted a correlation between frequently correct labels and preferred option positions, performance would be expected to regress toward random chance rather than fall substantially below it. Instead, the lowest-performing condition achieves only 26.08% accuracy, indicating systematic misclassification.\n",
    "\n",
    "These findings suggest that the performance drop induced by shuffling cannot be attributed to ground truth imbalance or trivial positional bias alone. Rather, the model appears to rely on learned associations between linguistic structure, option ordering, and expected answer semantics. When this implicit structure is disrupted, the model continues to apply these heuristics, resulting in confident but consistently incorrect predictions.\n",
    "\n",
    "**Per-Relation Accuracy and Asymmetric Spatial Competence**\n",
    "\n",
    "To further investigate the cause of the performance degradation under shuffled prompt conditions, we analyzed model accuracy conditioned on the ground-truth spatial relation. Although the dataset is nearly perfectly balanced across relations, model performance is highly asymmetric. Accuracy for the “Left” relation reaches 77.62%, whereas performance for “Right”, “On top”, and “Under” remains substantially lower, ranging from 36.55% to 43.21%.\n",
    "\n",
    "This discrepancy indicates that LLaVA has learned a strong bias or heuristic favoring the “Left” relation, independent of its true frequency in the dataset. Consequently, overall performance in structured multiple-choice settings may be artificially inflated when prompt ordering aligns with this internal bias. When answer options are shuffled, this alignment is disrupted, revealing the model’s limited ability to generalize spatial reasoning beyond its strongest learned association.\n",
    "\n",
    "Importantly, because all spatial relations occur with nearly equal frequency, the observed below-chance accuracy in certain shuffled conditions cannot be attributed to dataset imbalance. Instead, it reflects the model’s reliance on asymmetric, relation-specific heuristics rather than robust visual grounding. This finding explains why shuffling answer options can reduce performance to well below random chance while simultaneously increasing model confidence: the model applies a consistent but incorrect decision rule when its strongest prior (“Left”) is no longer structurally favored.\n",
    "\n",
    "![GroundTruthAccuracy](MultipleChoiceAccuracyPerGroundTruthOption.png)\n",
    "\n",
    "**Implications and Next Experimental Steps**\n",
    "\n",
    "The preceding analyses establish that LLaVA’s performance on simple spatial reasoning tasks is highly sensitive to prompt structure, particularly in multiple-choice settings. Although the dataset is nearly perfectly balanced across spatial relations, accuracy varies substantially across relations, with the model exhibiting markedly higher competence for “Left” than for “Right,” “On top,” or “Under.” When answer option ordering is altered, this asymmetry leads to systematic failure modes, including accuracy far below random chance and increased confidence in incorrect predictions. These results indicate that the model relies on stable linguistic heuristics rather than robust visual grounding.\n",
    "\n",
    "Having identified the fragility of multiple-choice prompting and its susceptibility to ordering effects, further ablations within this prompt family are unlikely to yield additional insight into the model’s core spatial reasoning abilities. Instead, the next phase of this work fixes the multiple-choice prompt as a baseline and shifts focus toward evaluating alternative prompting strategies that explicitly guide perception and reasoning. The goal is to assess whether structured prompts can reduce reliance on superficial textual shortcuts and encourage greater utilization of visual information.\n",
    "\n",
    "Specifically, we compare standard multiple-choice prompting against Chain-of-Thought prompting, Scene-Graph-based Chain-of-Thought prompting, and multi-stage descriptive prompting. These approaches differ in how explicitly they separate visual perception from reasoning and how strongly they constrain the model to attend to spatial relationships in the image. By evaluating these prompting strategies under identical visual and task conditions, we aim to determine whether structured reasoning prompts can meaningfully improve spatial understanding and mitigate the failure modes observed in multiple-choice formats.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac4f98",
   "metadata": {},
   "source": [
    "**NOTES**\n",
    "\n",
    "overall accuracy 49.58%\n",
    "overall confidence 47.56%   \n",
    "Multiple Choice: Acc=58.37%, Conf=47.56%\n",
    "Multiple Choice Shuffle: Acc=45.93%, Conf=50.54%\n",
    "Multiple Choice Shuffle Vary Above: Acc=26.08%, Conf=59.96%\n",
    "Multiple Choice Shuffle Vary Below: Acc=48.33%, Conf=49.72%\n",
    "Multiple Choice Shuffle Vary Under: Acc=54.55%, Conf=45.92%\n",
    "Multiple Choice Vary Above: Acc=55.02%, Conf=44.75%\n",
    "Multiple Choice Vary Below: Acc=54.07%, Conf=49.03%\n",
    "Multiple Choice Vary Under: Acc=54.31%, Conf=52.97%\n",
    "\n",
    "\t•\tLeft (Option A): 77.62%\n",
    "\t•\tRight (Option B): 36.55%\n",
    "\t•\tOn top (Option C): 43.21%\n",
    "\t•\tUnder (Option D): 40.78%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf95c18",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c25ac9",
   "metadata": {},
   "source": [
    "From these experiments, we conclude that LLaVA’s spatial reasoning is highly sensitive to prompt phrasing and option ordering. Baseline multiple-choice prompts with clear, unshuffled options provide the strongest performance. Shuffling or ambiguous phrasing can reduce accuracy drastically, demonstrating the model’s reliance on textual patterns rather than visual reasoning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLaVA venv)",
   "language": "python",
   "name": "llava-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

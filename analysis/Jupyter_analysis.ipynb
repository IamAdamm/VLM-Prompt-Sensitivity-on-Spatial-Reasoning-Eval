{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af724c2e",
   "metadata": {},
   "source": [
    "# Evaluating Prompt Sensitivity for VLMs on Simple Spatial Reasoning: A Controlled Study on LLaVA\n",
    "###### 20.12.2025\n",
    "#### Notebook by [Adam Astamir](https://adamastamir.vercel.app/)\n",
    "##### Supported by [Jae Hee Lee](https://jaeheelee.gitlab.io/)\n",
    "##### [University of Hamburg](https://www.uni-hamburg.de/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e2ff4",
   "metadata": {},
   "source": [
    "## Abstract "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333e6635",
   "metadata": {},
   "source": [
    "Vision–Language Models (VLMs), including LLaVA, have advanced considerably in multimodal understanding, but they continue to exhibit weaknesses in simple spatial reasoning tasks, such as identifying whether an object is to the left, right, above, or below another object. This study investigates how sensitive VLM performance is to prompt design, including prompt structure, option ordering, and wording. We hypothesize that spatial reasoning accuracy significantly varies depending on prompt clarity and phrasing, with clear, well-structured prompts yielding higher performance than ambiguous ones. To test this, we use part of the WhatsUp dataset, which contains 205 images presented in four controlled spatial positions, totaling 820 images, out of which we use 418 from control group 2. The dataset isolates basic spatial relations while minimizing background biases and confounding object-context associations. We evaluate multiple prompting strategies, including baseline multiple-choice prompts, we test for shuffled and wording-variant prompts, Chain-of-Thought (CoT) prompting and Scene-Graph-based CoT. Accuracy, prompt sensitivity, and error patterns are measured to assess how subtle variations in prompt design influence VLM reasoning. Preliminary analyses indicate that even small changes in phrasing or option ordering can have notable effects on model performance, supporting the hypothesis that VLM spatial reasoning is highly prompt-sensitive. The study aims to provide a controlled, systematic evaluation of prompt effects on spatial reasoning, quantify performance variations, and identify strategies that reliably improve VLM reasoning in simple spatial tasks. \n",
    "\n",
    "Results indicate that prompts are highly sensitive to option order and wording, and structured reasoning prompts improve accuracy significantly, with Scene-Graph CoT achieving the highest performance by far."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1853b",
   "metadata": {},
   "source": [
    "## Literature Review "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f696f",
   "metadata": {},
   "source": [
    "Spatial reasoning presents a persistent challenge for vision-language models (VLMs) because it requires the integration of visual information with textual cues in a coherent way. Many studies have shown that models like LLaVA tend to over-rely on textual prompts, often underutilizing the visual context. In some cases, performance actually improves when visual input is removed, highlighting the difficulty these models face in correctly aligning image representations with language understanding. Misaligned attention mechanisms further contribute to errors, leading to what has been described as “spatial hallucinations,” where objects’ relative positions are misinterpreted even in simple, controlled scenes.\n",
    "\n",
    "The WhatsUp dataset, introduced in 2023, provides a benchmark for evaluating basic spatial relations, such as left/right, on/under, and in front/behind. Its controlled design minimizes background distractions and isolates object positions, making it ideal for assessing prompt sensitivity. Empirical results on WhatsUp indicate that VLMs often perform near chance levels, around 50%, and subtle variations in wording can have a substantial effect on accuracy. For example, asking about an object “behind” another yields only 52% accuracy, whereas phrasing it as “in the background” increases performance to 67%. These results illustrate how linguistic framing alone can significantly influence model behavior, even for straightforward spatial tasks.\n",
    "\n",
    "Further evidence comes from the SpatialEval study, which shows that LLaVA and other VLMs frequently rely more on textual than visual information, often ignoring image content entirely. The study demonstrates that prompt phrasing and structure materially affect model performance, underscoring the importance of controlled experiments to isolate the effect of prompt design.\n",
    "\n",
    "Attempts to improve reasoning through Chain-of-Thought (CoT) prompting—guiding the model to think step by step—have yielded mixed results. Naive CoT prompts often fail to improve performance and can even degrade accuracy. In contrast, approaches that explicitly separate perception from reasoning, such as Scene-Graph-based CoT prompting, have been shown to enhance spatial understanding significantly. By first describing all objects and their spatial relationships, these methods help models interpret visual information more effectively, reducing errors caused by superficial linguistic biases.\n",
    "\n",
    "Recent work examining LLaVA’s attention patterns offers additional insight. Despite image tokens comprising approximately 90% of the input, they receive only about 10% of the model’s attention, indicating a strong bias toward textual priors. Techniques such as ADAPTVIS, which use confidence-guided attention, can partially correct this imbalance and improve spatial reasoning accuracy. This highlights that prompt design alone is insufficient; effective evaluation of spatial reasoning also requires attention to how visual information is processed internally.\n",
    "\n",
    "Overall, the literature suggests that spatial reasoning in VLMs is highly sensitive to prompt phrasing, structure, and ordering. Unstructured or ambiguous prompts frequently lead to poor performance, whereas structured, multi-stage, or descriptive prompts improve outcomes by explicitly guiding models to consider visual and spatial relationships. However, there remains a lack of systematic investigation into how multiple-choice variations, ordering, and alternative wordings influence simple spatial reasoning in LLaVA. This gap motivates the controlled experiments presented in this work, focusing on isolating the effects of prompt design on model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a8422",
   "metadata": {},
   "source": [
    "## Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de177cac",
   "metadata": {},
   "source": [
    "Our initial experiments focused exclusively on multiple-choice prompts to identify a strong baseline before testing more complex prompting strategies. Using the WhatsUp controlled dataset images, we evaluated variations in prompt wording and the ordering of multiple-choice options. This approach allows us to isolate the effects of text phrasing and option positioning on LLaVA’s spatial reasoning.\n",
    "\n",
    "We tested a range of prompting strategies including simple baseline instructions (“Which object is to the left of X?”), alternative wordings (\"under\" versus “underneath\", “above” versus “on top”), and shuffled answer options. Multiple-choice was selected as a baseline because it is easy to test and was used in the original WhatsUp paper. Chain-of-Thought and Scene-Graph CoT were selected because prior studies demonstrated that structured reasoning significantly improves spatial performance over naive CoT, with Scene-Graph CoT consistently outperforming all other types of CoT prompting. Our goal was to determine which combination of wording and ordering yields the highest accuracy so that subsequent experiments with Chain-of-Thought and Scene-Graph-based CoT can be compared to a strong multiple-choice baseline and use the most performant option of order and wording. Metrics for evaluation included accuracy, prompt sensitivity, confidence, and error type. All experiments were run with a batch size of 32 images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e47e10",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e1a03",
   "metadata": {},
   "source": [
    "Testing LLaVA with multiple-choice prompts and their variations, we observed a wide range of accuracies, indicating significant prompt sensitivity. The baseline multiple-choice prompt achieved 58.37% accuracy with 47.56% confidence, while shuffling the answer options reduced performance to 45.93% with 50.54% confidence. Some combinations of shuffled options and altered wording, such as “Shuffle Vary Above,” produced extremely low accuracy at 26.08% with 59.96% confidence, showing that subtle changes in both phrasing and ordering can drastically impair performance. Other variations, such as “Shuffle Vary Below” or “Shuffle Vary Under,” achieved 48.33% and 54.55%, respectively. Alternative wording alone, without shuffling, resulted in modest changes compared to the baseline, with “Vary Above” at 55.02%, “Vary Below” at 54.07%, and “Vary Under” at 54.31%. These results indicate that LLaVA is highly sensitive to option order and wording, with shuffling having a particularly strong negative impact.\n",
    "\n",
    "For Chain-of-Thought and Scene Graph CoT prompting, we first elicit structured reasoning from the model and then extract the final decision as one of the target spatial relations (“Left”, “Right”, “On top”, “Under”). This follows established evaluation protocols in recent spatial reasoning benchmarks, where the reasoning path is allowed to be free-form and the final decision is matched semantically. Multiple Choice prompting retains a fixed mapping to ensure baseline comparability. This approach yielded an accuracy of 66.03% with a confidence of 68.95%, showing improvements over multiple-choice prompts, particularly for spatial relations that were previously problematic. Scene-Graph CoT, which explicitly constructs a scene graph of objects and relations before reasoning, achieved the highest accuracy of 70.10% and a confidence of 60.99%. These results indicate that structured reasoning, especially when guided by scene graphs, can substantially improve spatial reasoning performance in LLaVA, reducing reliance on textual heuristics and first-option bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84450794",
   "metadata": {},
   "source": [
    "## Results & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f27080",
   "metadata": {},
   "source": [
    "Across all evaluated prompt variants, LLaVA achieved an overall accuracy of 49.58%, with an average reported confidence of 47.56%. Given the binary nature of the spatial reasoning task, this overall performance lies close to random chance. However, aggregate accuracy masks substantial variation across prompt configurations, indicating systematic sensitivity to prompt structure rather than random guessing.\n",
    "\n",
    "![Accuracy 49.58%](MultipleChoiceAccuracy.png)\n",
    "\n",
    "**Effect of Multiple-Choice Formatting and Option Order**\n",
    "\n",
    "The standard multiple-choice format achieved the highest accuracy (58.37%), suggesting that under stable and familiar prompt structures, LLaVA can perform above chance on simple spatial reasoning tasks. In contrast, shuffling the order of answer options led to a notable drop in accuracy (45.93%), despite a slightly higher average confidence (50.54%). This degradation implies that answer option order plays a significant role in model behavior.\n",
    "\n",
    "The impact of shuffling becomes particularly pronounced when combined with subtle wording changes. The “Multiple Choice Shuffle Vary Above” condition yielded an accuracy of only 26.08%, far below random chance, while simultaneously exhibiting the highest confidence across all conditions (59.96%). Such performance cannot be explained by uncertainty or noise; instead, it indicates the consistent application of an incorrect decision rule.\n",
    "\n",
    "Importantly, this effect is not uniform across all wording variations. While shuffling combined with “below” wording resulted in near-chance performance (48.33%), shuffling combined with “under” performed substantially better (54.55%). This non-linear interaction suggests that option order and linguistic phrasing jointly influence the model’s internal heuristics.\n",
    "\n",
    "**Effect of Wording Without Shuffling**\n",
    "\n",
    "When alternative spatial wording was introduced without shuffling the answer options, performance remained relatively stable. Accuracy across the “Vary Above”, “Vary Below”, and “Vary Under” conditions ranged from 54.07% to 55.02%, only slightly below the baseline multiple-choice condition. This indicates that wording variations alone do not severely impair performance, provided the overall prompt structure remains familiar.\n",
    "\n",
    "Taken together, these findings suggest that LLaVA is more sensitive to disruptions in answer ordering than to semantic variations in spatial terminology. Crucially, it is the interaction between wording and ordering—not either factor in isolation—that produces the most severe failure modes.\n",
    "\n",
    "**Confidence–Accuracy Mismatch and Systematic Failure**\n",
    "\n",
    "One of the most striking observations is the divergence between confidence and accuracy in certain conditions. The lowest-performing configuration (“Multiple Choice Shuffle Vary Above”) exhibits the highest confidence, indicating that the model is not merely guessing but is confidently applying an incorrect heuristic. This pattern suggests that model confidence reflects internal linguistic certainty rather than successful visual grounding.\n",
    "\n",
    "Accuracy falling well below chance further supports this interpretation. If the model were uncertain, performance would be expected to regress toward random guessing. Instead, the observed results indicate systematic misclassification driven by consistent but flawed decision rules.\n",
    "\n",
    "**Hypothesis: Interaction Between Spatial Direction and First-Option Bias**\n",
    "\n",
    "A plausible explanation for the observed sensitivity to shuffling involves positional bias in multiple-choice selection. Prior work on language models has documented a tendency to favor the first option in a list. In the non-shuffled baseline condition, the correct answer may more frequently correspond to the first option when the spatial relation involves “left”, due to the natural ordering of answer choices.\n",
    "\n",
    "Under this hypothesis, shuffling disrupts an implicit alignment between spatial direction (e.g., “left”) and option position, causing the model to apply a learned positional heuristic that no longer corresponds to the correct answer. This could explain why shuffling alone reduces performance, and why the combination of shuffling with specific wording (notably “above”) leads to severe degradation.\n",
    "\n",
    "Importantly, this interpretation predicts that the observed failure is not due to an inability to understand spatial relations per se, but rather due to the model over-relying on linguistic and positional shortcuts that correlate with correctness in the unshuffled setting.\n",
    "\n",
    "**Ground Truth Distribution and Positional Bias Analysis**\n",
    "\n",
    "To evaluate whether the observed degradation under shuffled answer options could be explained by dataset bias, we analyzed the distribution of ground truth labels across all test cases. The dataset is nearly perfectly balanced, with each spatial relation (“left”, “right”, “on top”, “under”) occurring in approximately 25% of samples.\n",
    "\n",
    "This balance rules out label frequency as a confounding factor. If shuffling merely disrupted a correlation between frequently correct labels and preferred option positions, performance would be expected to regress toward random chance rather than fall substantially below it. Instead, the lowest-performing condition achieves only 26.08% accuracy, indicating systematic misclassification.\n",
    "\n",
    "These findings suggest that the performance drop induced by shuffling cannot be attributed to ground truth imbalance or trivial positional bias alone. Rather, the model appears to rely on learned associations between linguistic structure, option ordering, and expected answer semantics. When this implicit structure is disrupted, the model continues to apply these heuristics, resulting in confident but consistently incorrect predictions.\n",
    "\n",
    "**Per-Relation Accuracy and Asymmetric Spatial Competence**\n",
    "\n",
    "To further investigate the cause of the performance degradation under shuffled prompt conditions, we analyzed model accuracy conditioned on the ground-truth spatial relation. Although the dataset is nearly perfectly balanced across relations, model performance is highly asymmetric. Accuracy for the “Left” relation reaches 77.62%, whereas performance for “Right”, “On top”, and “Under” remains substantially lower, ranging from 36.55% to 43.21%.\n",
    "\n",
    "This discrepancy indicates that LLaVA has learned a strong bias or heuristic favoring the “Left” relation, independent of its true frequency in the dataset. Consequently, overall performance in structured multiple-choice settings may be artificially inflated when prompt ordering aligns with this internal bias. When answer options are shuffled, this alignment is disrupted, revealing the model’s limited ability to generalize spatial reasoning beyond its strongest learned association.\n",
    "\n",
    "Importantly, because all spatial relations occur with nearly equal frequency, the observed below-chance accuracy in certain shuffled conditions cannot be attributed to dataset imbalance. Instead, it reflects the model’s reliance on asymmetric, relation-specific heuristics rather than robust visual grounding. This finding explains why shuffling answer options can reduce performance to well below random chance while simultaneously increasing model confidence: the model applies a consistent but incorrect decision rule when its strongest prior (“Left”) is no longer structurally favored.\n",
    "\n",
    "![GroundTruthAccuracy](MultipleChoiceAccuracyPerGroundTruthOption.png)\n",
    "\n",
    "**Implications and Next Experimental Steps**\n",
    "\n",
    "The preceding analyses establish that LLaVA’s performance on simple spatial reasoning tasks is highly sensitive to prompt structure, particularly in multiple-choice settings. Although the dataset is nearly perfectly balanced across spatial relations, accuracy varies substantially across relations, with the model exhibiting markedly higher competence for “Left” than for “Right,” “On top,” or “Under.” When answer option ordering is altered, this asymmetry leads to systematic failure modes, including accuracy far below random chance and increased confidence in incorrect predictions. These results indicate that the model relies on stable linguistic heuristics rather than robust visual grounding.\n",
    "\n",
    "Having identified the fragility of multiple-choice prompting and its susceptibility to ordering effects, further ablations within this prompt family are unlikely to yield additional insight into the model’s core spatial reasoning abilities. Instead, the next phase of this work fixes the multiple-choice prompt as a baseline and shifts focus toward evaluating alternative prompting strategies that explicitly guide perception and reasoning. The goal is to assess whether structured prompts can reduce reliance on superficial textual shortcuts and encourage greater utilization of visual information.\n",
    "\n",
    "Specifically, we compare standard multiple-choice prompting against Chain-of-Thought prompting and Scene-Graph-based Chain-of-Thought prompting. These approaches differ in how explicitly they separate visual perception from reasoning and how strongly they constrain the model to attend to spatial relationships in the image. By evaluating these prompting strategies under identical visual and task conditions, we aim to determine whether structured reasoning prompts can meaningfully improve spatial understanding and mitigate the failure modes observed in multiple-choice formats.\n",
    "\n",
    "**Structured Reasoning Experiments: Chain-of-Thought and Scene-Graph CoT**\n",
    "\n",
    "Building on the multiple-choice analysis, we implemented structured reasoning prompts to test whether explicitly guiding the model’s thought process could improve spatial understanding. The goal was to determine whether separating perception from reasoning reduces reliance on textual heuristics and first-option biases that dominated multiple-choice performance.\n",
    "\n",
    "For Chain-of-Thought (CoT) prompting, the model was instructed to verbalize its reasoning step by step before producing a final answer. This involved asking LLaVA to describe spatial relationships among objects in the image in a sequential, logical manner. The reasoning path was unconstrained in language but required the model to explicitly consider each object’s position relative to the target object. After reasoning, the final decision was mapped to one of the target spatial relations: “Left,” “Right,” “On top,” or “Under.”\n",
    "\n",
    "Scene-Graph CoT prompting added an intermediate step of explicitly constructing a scene graph of objects and their spatial relations. LLaVA first enumerated all objects in the image and recorded pairwise relationships, such as Object A being to the left of Object B or Object C being on top of Object D. This structured representation was then used to derive the final spatial answer. By forcing the model to encode object relations in a clear, structured form, Scene-Graph CoT reduces reliance on superficial textual patterns and encourages true visual reasoning.\n",
    "\n",
    "Both strategies were evaluated using the same WhatsUp control dataset images as the multiple-choice experiments. Metrics captured included final answer accuracy, confidence, and the ability to correctly resolve previously problematic relations, such as “Under,” which suffered under multiple-choice shuffling.\n",
    "\n",
    "Structured reasoning was motivated by the need to reduce shortcut reliance and encourage explicit consideration of visual evidence. Stepwise reasoning with CoT prompts forces the model to account for each object in relation to the target, while scene graphs formalize perception by encoding all object–relation pairs before producing a final answer. Evaluating these approaches directly against the multiple-choice baseline allows assessment of how structured reasoning impacts accuracy and mitigates prior biases.\n",
    "\n",
    "The results demonstrate clear improvements. Chain-of-Thought prompting increased overall accuracy from the multiple-choice baseline of roughly 58–59% to 66.03%, with confidence reaching 68.95%, particularly improving performance on relations that were previously affected by shuffling or wording changes. Scene-Graph CoT achieved the highest accuracy at 70.10% with confidence of 60.99%, showing that explicitly modeling object relations is more effective than unconstrained stepwise reasoning. Both structured approaches substantially reduced first-option biases, improving performance for underperforming relations such as “Under.” Furthermore, confidence–accuracy alignment improved compared to multiple-choice prompts, indicating that structured reasoning better reflects true understanding rather than heuristic certainty.\n",
    "\n",
    "Overall, structured reasoning, and especially Scene-Graph CoT, effectively mitigates the failure modes identified in multiple-choice experiments, supporting the hypothesis that visual reasoning improves when perception and relational inference are explicitly separated.\n",
    "\n",
    "![AccuracyAllPrompts](AllPromptsAccuracy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66a16b",
   "metadata": {},
   "source": [
    "![GroundTruthAccuracy2](AllPromptsAccuracyPerGroundTruthOption.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf95c18",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c25ac9",
   "metadata": {},
   "source": [
    "From these experiments, we conclude that LLaVA’s spatial reasoning is highly sensitive to prompt phrasing and option ordering. Baseline multiple-choice prompts with clear, unshuffled options provide the strongest performance. Shuffling or ambiguous phrasing can reduce accuracy drastically, demonstrating the model’s reliance on textual patterns rather than visual reasoning.\n",
    "\n",
    "Structured reasoning prompts such as Chain-of-Thought and Scene-Graph CoT improve performance, with Scene-Graph CoT achieving the highest accuracy by explicitly modeling spatial relationships in two steps.\n",
    "\n",
    "Limitations \n",
    "include the use of a controlled subset of WhatsUp images, which may not generalize to more complex or naturalistic scenes. The model’s attention distribution remains biased toward text, and confidence does not always align with accuracy, particularly for challenging spatial relations such as “Under.” Future work should expand datasets, examine more diverse prompting strategies, and explore techniques to better integrate visual information in reasoning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLaVA venv)",
   "language": "python",
   "name": "llava-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

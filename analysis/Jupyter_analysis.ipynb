{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af724c2e",
   "metadata": {},
   "source": [
    "# Evaluating Prompt Sensitivity for VLMs on Simple Spatial Reasoning: A Controlled Study on LLaVA\n",
    "###### 20.12.2025\n",
    "#### Notebook by [Adam Astamir](https://adamastamir.vercel.app/)\n",
    "##### Supported by [Jae Hee Lee](https://jaeheelee.gitlab.io/)\n",
    "##### [University of Hamburg](https://www.uni-hamburg.de/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d60bf9",
   "metadata": {},
   "source": [
    "| Paper | Model(s) | Task Type | Prompt Role | Key Failure | What’s Missing |\n",
    "|------|----------|-----------|-------------|-------------|---------------|\n",
    "| What’s “Up” with Vision-Language Models? (2023) | 18 VLMs (CLIP, BLIP, BLIP-2, XVLM, CoCa, FLAVA...) | Image–text matching for basic spatial relations (left/right, on/under, in front/behind) | Minimal. Prompting controlled for isolation of spatial reasoning | Models fail to reliably distinguish basic spatial relations. performance often near random |  Focus on prompting not given. Also no Counting involved   |\n",
    "| Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models (2024)| 7 VLMs (GPT-4V, **LLaVA**, BLIP-2, MiniGPT-4, OpenFlamingo, InstructBLIP, Otter) |  Multiple-choice spatial reasoning tasks (e.g., map navigation, object arrangement, grid puzzles)  | Moderate. Text prompts guide reasoning, images provide context | VLMs often ignore visual input or rely too heavily on text. accuracy sometimes at or below random chance; struggles with multi-step spatial reasoning  | No experiments on CLIP specifically. Synthetic image used may not include biases of real world examples  |\n",
    "| Enhancing Spatial Reasoning in Vision-Language Models via CoT Prompting & RL (2025) | PaLI-Gemma2, Qwen2.5-VL, Llama-4-Scout |  Counting, Relations, Depth, Distance, Egocentric/Object Movement (SAT, CV-Bench, CLEVR, VSR) | All about prompting. Many different techniques |  Naïve CoT can harm performance; models may rely on superficial linguistic patterns; standard SFT fails to generalize OOD | No LlaVA and little CLIP metion  |\n",
    "| Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas (2025) | **LLaVA-1.5, LLaVA-1.6**, Qwen2-VL | Simple Multiple-choice spatial reasoning tasks (left/right, on/under, front/behind) from WhatsUp & VSR datasets  | Moderate. Prompts ask for object relations; adaptive attention (ADAPTVIS) uses model confidence to guide reasoning |  VLMs underutilize image tokens; attention misalignment causes spatial hallucinations; model over-relies on familiar relationships; low-confidence relations fail |  Focus on CLIP as backbone is limited; dependency on validation sets for ADAPTVIS; only intermediate layers analyzed; specific prompt effects not deeply varied |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847e2ff4",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7afa3d5a",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c31366",
   "metadata": {},
   "source": [
    "WRITE AN INTORDUTCION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47055afd",
   "metadata": {},
   "source": [
    "### Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01222f0c",
   "metadata": {},
   "source": [
    "How sensitive is spatial reasoning performance in VLMs to prompt structure & ordering?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869b29a",
   "metadata": {},
   "source": [
    "### Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea9b03c",
   "metadata": {},
   "source": [
    "Spatial reasoning performance of VLMs significantly varies depending on structure and order of prompts, such that prompts with clear instructions yield higher accuracy than with ambiguous or shuffled instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e4e4bd",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e1edba",
   "metadata": {},
   "source": [
    "Write methods here\n",
    "used Whatsup dataset of 205 images in 4 differing positions totaling 820 images because it isolates simple spatial relations like left or right much more precisely than most existing datasets - The images in the dataset are exactly the same with minimized background, except that the objects being tested move around to have differing spatial relations to one another and also account for special biases like a dog usually being under the table instead also being placed on top or objects like house being above water instead of under it not being used. Whatsup also shows that the word \"behind\"(52% accuracy) is not as performant as \"in the background\" (67% accuracy).\n",
    "\n",
    "WHAT TO TEST? (only simple spatial relations?)\n",
    "CoT prompting, VoT prompting?, CoT with Scene Graph prompting!, Multiple Choice prompting, Shuffled Prompting, Simple prompting, different Words like backround instead of behind prompting, Describe CoT (First describe Objects then spatial relations prompting), optical flow Cot?, Structured few-Shot promting?, Conversational few-Shot prompting?,\n",
    "\n",
    "WILL TEST:\n",
    "Simple instruction to serve as baseline and reflect naive prompting (Which object is to the left of X? Where is X in relation to the object?...)\n",
    "Chain of Thought prompting (Think step by step, (insert basic prompt))\n",
    "Scene Graph CoT (First list all objects and their relative positions, then answer: (insert basic prompt)) (Literature suggests this is way better than CoT and CoT may even hurt performance and also this is way better generally)\n",
    "Multi-stage descriptive prompting (Describe all objects in the scene and their colors/positions. Now answer: (Insert basic prompt)) (Paper suggests that explicit separation of perception and reasoning may improve accuracy)\n",
    "Multiple Choice Prompts((insert basice prompt) Choose one of the following: ...)\n",
    "Alternative Wording! (\"behind\" vs \"in the background\", \"On top\" vs \"above\"...) (Whatsup suggests Wording changes performance significantly (behind = 52%, in the background = 67%))\n",
    "Shuffled or ambiguous prompts (Left,Right,Above... -> Right,Above,Left... , is Left of table to left of me or left of table meant?) (Tests sensitivity to ordering and clarity, directly addressing hypothesis)\n",
    "Few-Shot Prompting (“Earlier, you identified positions like this: (prev answer). Now answer this one (insert basic prompt).”) (Could improve reasoning by giving context, paper suggests this)\n",
    "\n",
    "Metrics defining success:\n",
    "Accuracy in % (Primary) (obvious)\n",
    "Prompt sensitivity (Secondary) (Measures how much prompting mattered)\n",
    "Confidence (Secondary) (Insight into whether attention or confidence signals align with spatial reasoning performance. Also can talk about newest Paper ADAPTVIS)\n",
    "Error Type (Secondary) (Tells me if left or right is more performant than above or below)\n",
    "\n",
    "Will test on all of Whatsup Images Dataset consisting of 205 images in 4 different spatial relations totaling 820 images. Batch size will be 32. This is fine because Whatsup images are nice as mentioned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dd9f44",
   "metadata": {},
   "source": [
    "### Research Goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f773b9",
   "metadata": {},
   "source": [
    "To systematically evaluate the effect of prompt design on VLM spatial reasoning performance, identify limitations, and provide insights for more effective prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b448fed",
   "metadata": {},
   "source": [
    "### Key findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f997aec",
   "metadata": {},
   "source": [
    "Can be empty for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a1853b",
   "metadata": {},
   "source": [
    "## Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67f696f",
   "metadata": {},
   "source": [
    "explin why spatial reasoning is hard and give some summary of relevant papers include links?\n",
    "Whatsup Paper\n",
    "Whatsup suggests that prompting does impact performance significantly and also shows that spatial relations performance generally lands at around 50% suggesting that its barely better than guessing and sometimes even lands at less than 50% meaning its systematically guessing wrong - probable cause would the biases like a cup usually being on top of a table instead of under.\n",
    "SpatialEval Paper\n",
    "The SpatialEval paper suggests that VLMs, including LLaVA, heavily rely on textual cues and often underutilize visual input—sometimes performing better without any visual cues at all. It provides both a benchmark framework and concrete evidence that LLaVA’s spatial reasoning is highly sensitive to textual information and task setup, supporting a controlled analysis of how different prompts influence performance on spatial tasks.\n",
    "CoT Paper\n",
    "The statement \"our empirical results show that naive CoT prompting (e.g., ”think first, then answer”) not only fails to improve spatial reasoning in VLMs, but may even degrade performance.\" supports importance of prompting for spatial relations.\n",
    "CoT worsens performance and Scene-Graph based CoT significantly improves it (also reduces the chance of reward hacking maybe?).\n",
    "Using the prompt alongside the image or only the prompt increases accuracy supporting hypothesis. \n",
    "the paper Systematically evaluates how different prompting strategies affect spatial reasoning in VLMs. It shows that naive prompting (like simple CoT) can degrade performance, while structured prompts based on scene graphs and multi-stage approaches improve accuracy. The use of few-shot and conversational prompting highlights how subtle changes in prompt design can influence reasoning outcomes.\n",
    "Additionally, the paper examines generalization under out-of-distribution conditions, showing that model performance can shift depending on prompt phrasing—critical evidence. Finally, the GRPO reinforcement learning fine-tuning results demonstrate that beyond prompt design, the model’s ability to align visual and textual cues also strongly impacts spatial reasoning performance, providing context for interpreting own experiments.\n",
    "Why is Spatial Reasoning hard for VLMs paper\n",
    "Paper specifically examines LLaVA and studies how prompting interacts with attention for spatial reasoning. It shows that prompt-based tasks alone are insufficient if the model misallocates attention; the key insight is that model confidence can guide adaptive attention (ADAPTVIS) to significantly improve performance. LLaVA struggles with spatial reasoning mainly due to sparse and misaligned attention on image tokens. Standard prompts guide reasoning moderately but cannot fix attention issues(relevant?). ADAPTVIS: confidence-guided attention intervention improves spatial reasoning up to 50 absolute points (relevant?).\n",
    "Intermediate layers of LLaVA are crucial for processing visual info; early layers capture global structure, middle layers refine spatial understanding (relevant?).\n",
    "Attention distribution, not just quantity, is critical. Confidence scores indicate familiarity with relations.\n",
    "\"Our investigation begins with a key observation: despite image tokens comprising around\n",
    "90% of the input sequence, they receive only about 10% of the model’s attention. This significant imbalance suggests that textual priors often overshadow visual evidence,\n",
    "explaining VLMs’ struggles with vision-centric tasks.\" statement suggests hypothesis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55a8422",
   "metadata": {},
   "source": [
    "## Experimental Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7c8952",
   "metadata": {},
   "source": [
    "WHOLE LOTTA STUFF HERE\n",
    "using the Whatsup dataset IMages. Why? PUT LINK HERE TOO\n",
    "\n",
    "first i took the multiple choice only parts and varied wording and shuffled too to test how order and wording impacts performance. Then I will take the highest performant multiple choice prompt and compare to other prompting types on performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df692ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Works\n"
     ]
    }
   ],
   "source": [
    "print(\"Works\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e47e10",
   "metadata": {},
   "source": [
    "## Actual Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e1a03",
   "metadata": {},
   "source": [
    "Experimetn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84450794",
   "metadata": {},
   "source": [
    "## Results & Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac4f98",
   "metadata": {},
   "source": [
    "![Accuracy](MultipleChoioceAccuracy.png)\n",
    "Above has lower accuracy for Shows that Different Wording impacts performance in a negative way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf95c18",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183ae2b7",
   "metadata": {},
   "source": [
    "COnlsulion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (LLaVA venv)",
   "language": "python",
   "name": "llava-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
